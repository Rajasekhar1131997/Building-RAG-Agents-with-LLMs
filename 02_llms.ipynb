{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ee3921-2244-4545-b0df-0b0ebebff32d",
   "metadata": {
    "id": "38ee3921-2244-4545-b0df-0b0ebebff32d"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zTeQZUUuG1u1",
   "metadata": {
    "id": "zTeQZUUuG1u1"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 2:** LLM Services and AI Foundation Models</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this notebook, we will explore LLM services! We'll discuss the reasons for and against deploying LLMs on edge devices alongside ways to deliver powerful models to end users through scalable server deployments like those accessible through the NVIDIA AI Foundation Endpoints.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Understanding the pros and cons of running LLM services locally vs in a scalable cloud environment.\n",
    "- Getting familiar with the AI Foundation Model Endpoint schemes, including:\n",
    "    - The raw low-level connection interface facilitated by packages like `curl` and `requests`\n",
    "    - The abstractions created to make this interface function seamlessly with open-sourced software like LangChain.\n",
    "- Getting comfortable with retrieving LLM generations from the pool of endpoints and being able to select a subset of models to build your software on.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "1. What kind of model access should you give a person developing an LLM stack, and how does it compare to the access you need to provide to end-users of an AI-powered web application?\n",
    "2. When considering which devices to support, what kinds of rigid assumptions are you making about their local compute resources and what types of fallbacks should you implement?\n",
    "    - What if you wanted to deliver a jupyter labs interface with access to a private LLM deployment to customers.\n",
    "    - What if now you wanted to support their local jupyter lab environment with your private LLM deployment?\n",
    "    - Would anything have to change if you decided to support embedded devices (i.e. Jetson Nano)?\n",
    "3. **[Harder]** Assume you have Stable Diffusion, Mixtral, and Llama-13B deployed on your own compute instance in a cloud environment sharing the same GPU resource. You currently do not have a business use case for Stable Diffusion, but your teams are experimenting with the other two for LLM applications. Should you remove Stable Diffusion from your deployment?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc2467-fce5-4cda-800d-3b24463a32f4",
   "metadata": {
    "id": "d7dc2467-fce5-4cda-800d-3b24463a32f4"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1**: Getting Large Models Into Your Environment\n",
    "\n",
    "Recall from the last notebook that our current environment has several microservices running on an allocated cloud instance: `docker-router`, `jupyter-notebook-server`, `frontend`, and `llm-service` (among others). \n",
    "\n",
    "- **jupyter-notebook-server**: The service that is running this jupyter labs session and hosting out python environment. \n",
    "- **docker_router**: A service to help us at least observe and monitor our microservices.\n",
    "- **frontend**: A live website microservice that delivers us a simple chat interface. \n",
    "\n",
    "This notebook will focus more on the `llm-service` microservice, which you will be using (at least under the hood) to interface with a selection of [**foundation models**](https://www.nvidia.com/en-us/ai-data-science/foundation-models/)! Specifically, you'll be using a subset of the [**NVIDIA AI Foundation Models**](https://catalog.ngc.nvidia.com) to prototype AI-enabled pipelines and orchestrate non-trivial natural-language-backed applications.\n",
    "\n",
    "$$---$$\n",
    "\n",
    "\n",
    "Across just about every domain, deploying massive deep learning models is a common yet challenging task. Today's models, such as Llama 2 (70B parameters) or ensemble models like Mixtral 7x8B, are products of advanced training methods, vast data resources, and powerful computing systems. Luckily for us, these models have already been trained and many use cases can already be achieved with off-the-shelf solutions. The real hurdle, however, lies in effectively hosting these models.\n",
    "\n",
    "**Deployment Scenarios for Large Models:**\n",
    "\n",
    "1. **High-End Datacenter Deployment:**\n",
    "> An uncompressed, unquantized model on a data center stack equipped with GPUs like NVIDIA's [A100](https://www.nvidia.com/en-us/data-center/a100/)/[H100](https://www.nvidia.com/en-us/data-center/h100/)/[H200](https://www.nvidia.com/en-us/data-center/h200/) to facilitate fast inference and experimentation.\n",
    "> - **Pros**: Ideal for scalable deployment and experimentation, this stack is ideal for either large training workflows or for supporting multiple users or models at the same time.  \n",
    "> - **Cons:** It is inefficient to allocate this resource for each user of your service unless the use cases involve model training/fine-tuning or interfacing with lower-level model components.\n",
    "\n",
    "2. **Modest Datacenter/Specialized Consumer Hardware Deployment:**\n",
    "> Quantized and further-optimized models can be run (one or two per instance) on more conservative datacenter GPUs such as [L40](https://www.nvidia.com/en-us/data-center/l40/)/[A30](https://www.nvidia.com/en-us/data-center/products/a30-gpu/)/[A10](https://www.nvidia.com/en-us/data-center/products/a10-gpu/) or even on some modern consumer GPUs such as the higher-VRAM [RTX 40-series GPUs](https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/).\n",
    "> - **Pros:** This setup balances inference speed with manageable limitations for single-user applications. These sessions can also be deployed on a per-user basis to run one or two large models at a time with raw access to model internals (even if they need quantization).\n",
    "> - **Cons:** Deploying an instance for each user is still costly at scale, though it may be justifiable for some niche workloads. Alternatively, assuming that users can access these resources in their local environments is likely unreasonable.\n",
    "\n",
    "3. **Consumer Hardware Deployment:**\n",
    "> Though heavily limited in ability to propagate data through a neural network, most consumer hardware does have a graphical user interface (GUI), a web browser with internet access, some amount of memory (can safely assume at least 1 GB), and a decently-powerful CPU.\n",
    "> - **Cons:** Most hardware at the moment cannot run more than one local large model at a time in any configuration, and running even one model will require significant amounts of resource management and optimizing restrictions.\n",
    "> - **Pros:** This is a reasonable and inclusive starting assumption when considering what kinds of users your services should support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dDqGtWuYwo9",
   "metadata": {
    "id": "8dDqGtWuYwo9"
   },
   "source": [
    "In this course, your environment will be quite representative of typical consumer hardware; though we can kickstart and prototype with microservices, we are constrained by a CPU-only compute environment that will struggle to run an LLM model. While this is a significant limitation, we will still be able to take advantage of fast LLM capabilities via:\n",
    "- Access to a compute-capable service for hosting large models.\n",
    "- A streamlined interface for command input and result retrieval.\n",
    "\n",
    "With our foundation in microservices and port-based connections, we are well-positioned to explore effective interfacing options for getting LLM access for our development environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154bced-9358-4e43-a85d-25eeef995f6a",
   "metadata": {
    "id": "f154bced-9358-4e43-a85d-25eeef995f6a"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** Hosted Large Model Services\n",
    "\n",
    "In our pursuit to provide access to Large Language Models (LLMs) in a resource-constrained environment like ours, characterized by CPU-only instances, we'll evaluate various hosting options:\n",
    "\n",
    "**Black-Box Hosted Models:**\n",
    "> Services such as [**OpenAI**](https://openai.com/) offer APIs to interact with black-box models like GPT-4. These powerful, well-integrated services can provide simple interfaces to complex pipelines that automatically track memory, call additional models, and incorporate multimodal interfaces as necessary to simplify typical use scenarios. At the same time, they maintain operational opacity and often lack a straightforward path to self-hosting.\n",
    "> - **Pros:** Easy to use out-of-the-box with shallow barriers to entry for an average user.\n",
    "> - **Cons:** Black-box deployments suffer from potential privacy concerns, limited customization, and cost implications at scale.\n",
    "\n",
    "**Self-Hosted Models:**\n",
    "\n",
    "> Behind the scenes of just about all scaled model deployments is one or more giant models running in a data center with scalable resources and lightning-fast bandwidth at their disposal. Though necessary to deploy large models at scale and maintain strong control over the provided interfaces, these systems often require expertise to set up and generally do not work well for supporting non-developer workflows for only one individual at a time. Such systems are much better for supporting many simultaneous users, multiple models, and custom interfaces.\n",
    "> - **Pros:** They offer the capability to integrate custom datasets and APIs and are primarily designed to support numerous users concurrently.\n",
    "> - **Cons:** These setups demand technical expertise to set up and properly configure.\n",
    "\n",
    "To get the best of both worlds, we will utilize the [**NVIDIA NGC Service**](https://www.nvidia.com/en-us/gpu-cloud/). NGC offers a suite of developer tools for designing and deploying AI solutions. Central to our needs are the [NVIDIA AI Foundation Models](https://www.nvidia.com/en-us/ai-data-science/foundation-models/), which are pre-tuned and pre-optimized models designed for easy out-of-the-box scalable deployment (as-is or with further customization). Furthermore, NGC hosts accessible model endpoints for querying live foundation models in a [scalable DGX-accelerated compute environment](https://www.nvidia.com/en-us/data-center/dgx-platform/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae66826-e829-42b4-a125-c533d2e6ffae",
   "metadata": {
    "id": "bae66826-e829-42b4-a125-c533d2e6ffae"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3:** Getting Started With Hosted Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2b1cf-9e8d-4687-822b-5bf8fbb4adb2",
   "metadata": {},
   "source": [
    "**When deploying a model for scaled inference, the steps you generally need to take are as follows:**\n",
    "- Identify the models you would like users to access, and allocate resources to host them.\n",
    "- Figure out what kinds of controls you would like users to have, and expose ways for them to access it.\n",
    "- Create monitoring schemes to track/gate usage, and set up systems to scale and throttle as necessary.\n",
    "\n",
    "For this course, you'll use the models deployed by NVIDIA, which are hosted as [**NVIDIA NIM Microservices**](https://www.nvidia.com/en-us/ai/). The NIM ecosystem consists of microservices that are optimized to run AI workloads for scaled inference deployment. They work just fine for local inference and offer standardized APIs, but are primarily designed to work especially well in scaled environments. These particular models are deployed on NVIDIA DGX Cloud as shared functions and are advertised through an OpenAPI-style API gateway. Let's unpack what that means:\n",
    "\n",
    "**On The Cluster Side:** These microservices are hosted on a Kubernetes-backed platform that scales the load across a minimum and maximum number of DGX Nodes and are delivered behind a single function. In other words:\n",
    "- A large-language model is downloaded to and deployed on a **GPU-enabled compute node** (i.e. a powerful CPU and 4xH100-GPU environment which is physically-integrated in a DGX Pod).\n",
    "- On start, a selection of these compute nodes are kickstarted such that, whenever a user sends a request to the function, one of those nodes will receive the request.\n",
    "    - Kubernetes will route this traffic appropriately. If there is an idle compute node, it will receive the traffic. If all of them are working, the request will be queued up and a node will pick it up as soon as possible.\n",
    "    - In our case, these nodes will still pick up requests very fast since in-flight batching is enabled, meaning each node can take in up to 256 active requests at a time as-they-come before they get completely \"full\". (256 is a hyperparameter on deployment).\n",
    "- As load begins to increase, auto-scaling will kick in and more nodes will be kickstarted to avoid request handling delays.\n",
    "\n",
    "The following image shows an arbitrary function invocation with a custom (non-OpenAPI) API. This was the initial way in which the public endpoints were advertised, but is now an implementation detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zbt4oxFFhwo-",
   "metadata": {
    "id": "Zbt4oxFFhwo-"
   },
   "source": [
    "<!-- > <img style=\"max-width: 1000px;\" src=\"imgs/ai-playground-api.png\" /> -->\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1ckAIZoy7tvtK1uNqzA9eV5RlKMbVqs1-\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/ai-playground-api.png\" width=800px/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae644b94-6566-4c3a-a87b-76c43230e0ab",
   "metadata": {},
   "source": [
    "**On The Gateway Side:** To make this API more standard, an API gateway server is used to aggregate these functions behind a common API known as OpenAPI. This specification is subscribed to by many including OpenAI, so using the OpenAI client is a valid interface: \n",
    "\n",
    "<!-- > <img style=\"max-width: 800px;\" src=\"imgs/mixtral_api.png\" /> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/mixtral_api.png\" width=800px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6f5c4-98d8-48cc-bc0f-5166519bdd39",
   "metadata": {},
   "source": [
    "For this course, you will want to use a more specialized interface that connects to an LLM orchestration framework called LangChain (more on that later). For on your end, you will be using the more tailored interface like `ChatNVIDIA` from the [`langchain_nvidia_ai_endpoints`](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) library. *More on that later.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131d872-c4c9-4395-9ac3-b54abb8c473f",
   "metadata": {},
   "source": [
    "**On The User Side:** Incorporating these endpoints into your client, you can design integrations, pipelines, and user experiences that leverage these generative AI capabilities to endow your applications with reasoning and generative abilities. A popular example of such an application is [**OpenAI's ChatGPT**](https://chat.openai.com/), which is an orchestration of endpoints including GPT4, Dalle, and others. Though it may sometimes look like a single intelligent model, it is merely an aggregation of model endpoints with software engineering to help manage state and context control. This will be reinforced throughout the course, and by the end you should have an idea for how you could go about making a similar chat assistant for an arbitrary use-case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0377e6a-0796-404d-a219-84e9b107c32c",
   "metadata": {},
   "source": [
    "<!-- > <img style=\"max-width: 700px;\" src=\"imgs/openai_chat.png\" /> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/openai_chat.png\" width=700px/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f329bd7-712a-4004-891f-e475eddef112",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4: [Exercise]** Trying Out The Foundation Model Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c32c86-ffaf-4335-ad2f-6d48d61c8577",
   "metadata": {},
   "source": [
    "In this section, you will finally get to interact with your LLM endpoints! \n",
    "\n",
    "**From Your Own Environment**: You would want to go to [`build.nvidia.com`](https://build.nvidia.com/) and find a model you'd like to use. For example, you could go to [**the MistralAI's Mixtral-8x7b model**](https://build.nvidia.com/mistralai/mixtral-8x7b-instruct) to see an example of how to use the model, links for further readings, and some buttons like \"Apply To Self-Host\" and \"Get API Key.\"\n",
    "\n",
    "- Clicking **\"Apply To Self-Host\"** will guide you to information about NVIDIA Microservices and give you some avenues to sign up (i.e. early access/NVIDIA AI Enterprise pathway) or enter a notification list (General Access pathway).\n",
    "\n",
    "- Clicking **\"Get API Key\"** will generate an API key starting with \"nvapi-\" which you can provide to the API endpoints via a network request!\n",
    "\n",
    "If you were to do this, you would need to add the API key to the notebook like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08c8c8-eb2d-4d56-a5c6-21e80fd1bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01c207-cbe1-4aa5-8b1c-e282c9a1730c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "**From Your Course Environment**: For the sake of the course, we will be directly using these models through a server set up in the `llm_client` directory (namely [**`llm_client/client_server.py`**](llm_client/client_server.py)). The details surrounding its implementation is outside of scope for the course, but will give you unlimited access to a selection of models by: \n",
    "- Exposing some endpoints that will pass your request to a selection of models.\n",
    "- Filling in an API key from within the llm_client microservice so that you don't run out of credits. \n",
    "\n",
    "***The same code can also be used as useful starting point for implementing your own GenAI gateway service like [`integrate.api.nvidia.com`](https://docs.api.nvidia.com/nim/reference/nvidia-embedding-2b-infer) or [`api.openai.com`](https://platform.openai.com/docs/api-reference/introduction).***\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388f206-5745-41aa-b5b5-a954e0e92029",
   "metadata": {},
   "source": [
    "### **4.1.** Manual Python Requests\n",
    "\n",
    "As we said before, you can interact with microservices or remote APIs using Python's `requests` library, and will generally follow the following process:\n",
    "- **Importing Libraries:** We start by importing requests for HTTP requests and json for handling JSON data.\n",
    "- **API URL and Headers:** Define the URL of the API endpoint and headers, including authorization (API key) and data format preferences.\n",
    "- **Data Payload:** Specify the data you want to send; here, itâ€™s a simple query.\n",
    "- **Making the Request:** Use `requests.post` to send a POST request. You can replace post with `get`, `put`, etc., depending on the API's requirements.\n",
    "- **Response Handling:** Check the status code to determine if the request was successful (200 means success) and then process the data.\n",
    "\n",
    "To establish a bit about the service, we can try to see what kinds of endpoints and models it provides: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f519964-5796-4073-9f9f-81037839c255",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='llm_client', port=9000): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001F2B4D9BA10>: Failed to resolve 'llm_client' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:445\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:1298\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1298\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:1058\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1058\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1061\u001b[0m \n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:996\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 996\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:276\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x000001F2B4D9BA10>: Failed to resolve 'llm_client' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='llm_client', port=9000): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001F2B4D9BA10>: Failed to resolve 'llm_client' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m invoke_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://llm_client:9000\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m----> 6\u001b[0m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvoke_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='llm_client', port=9000): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x000001F2B4D9BA10>: Failed to resolve 'llm_client' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "invoke_url = \"http://llm_client:9000\"\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "\n",
    "requests.get(invoke_url, headers=headers, stream=False).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835d15b7-eefe-4232-8536-5e72b2476a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models:\n",
      "<Response [200]>\n",
      " - 01-ai/yi-large\n",
      " - abacusai/dracarys-llama-3.1-70b-instruct\n",
      " - adept/fuyu-8b\n",
      " - ai21labs/jamba-1.5-large-instruct\n",
      " - ai21labs/jamba-1.5-mini-instruct\n",
      " - aisingapore/sea-lion-7b-instruct\n",
      " - baai/bge-m3\n",
      " - baichuan-inc/baichuan2-13b-chat\n",
      " - bigcode/starcoder2-15b\n",
      " - bigcode/starcoder2-7b\n",
      " - databricks/dbrx-instruct\n",
      " - deepseek-ai/deepseek-coder-6.7b-instruct\n",
      " - deepseek-ai/deepseek-r1\n",
      " - deepseek-ai/deepseek-r1-0528\n",
      " - deepseek-ai/deepseek-r1-distill-llama-8b\n",
      " - deepseek-ai/deepseek-r1-distill-qwen-14b\n",
      " - deepseek-ai/deepseek-r1-distill-qwen-32b\n",
      " - deepseek-ai/deepseek-r1-distill-qwen-7b\n",
      " - google/codegemma-1.1-7b\n",
      " - google/codegemma-7b\n",
      " - google/deplot\n",
      " - google/gemma-2-27b-it\n",
      " - google/gemma-2-2b-it\n",
      " - google/gemma-2-9b-it\n",
      " - google/gemma-2b\n",
      " - google/gemma-3-12b-it\n",
      " - google/gemma-3-1b-it\n",
      " - google/gemma-3-27b-it\n",
      " - google/gemma-3-4b-it\n",
      " - google/gemma-3n-e2b-it\n",
      " - google/gemma-3n-e4b-it\n",
      " - google/gemma-7b\n",
      " - google/paligemma\n",
      " - google/recurrentgemma-2b\n",
      " - google/shieldgemma-9b\n",
      " - gotocompany/gemma-2-9b-cpt-sahabatai-instruct\n",
      " - ibm/granite-3.0-3b-a800m-instruct\n",
      " - ibm/granite-3.0-8b-instruct\n",
      " - ibm/granite-3.3-8b-instruct\n",
      " - ibm/granite-34b-code-instruct\n",
      " - ibm/granite-8b-code-instruct\n",
      " - ibm/granite-guardian-3.0-8b\n",
      " - igenius/colosseum_355b_instruct_16k\n",
      " - igenius/italia_10b_instruct_16k\n",
      " - institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1\n",
      " - institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1\n",
      " - marin/marin-8b-instruct\n",
      " - mediatek/breeze-7b-instruct\n",
      " - meta/codellama-70b\n",
      " - meta/llama-3.1-405b-instruct\n",
      " - meta/llama-3.1-70b-instruct\n",
      " - meta/llama-3.1-8b-instruct\n",
      " - meta/llama-3.2-11b-vision-instruct\n",
      " - meta/llama-3.2-1b-instruct\n",
      " - meta/llama-3.2-3b-instruct\n",
      " - meta/llama-3.2-90b-vision-instruct\n",
      " - meta/llama-3.3-70b-instruct\n",
      " - meta/llama-4-maverick-17b-128e-instruct\n",
      " - meta/llama-4-scout-17b-16e-instruct\n",
      " - meta/llama-guard-4-12b\n",
      " - meta/llama2-70b\n",
      " - meta/llama3-70b-instruct\n",
      " - meta/llama3-8b-instruct\n",
      " - microsoft/kosmos-2\n",
      " - microsoft/phi-3-medium-128k-instruct\n",
      " - microsoft/phi-3-medium-4k-instruct\n",
      " - microsoft/phi-3-mini-128k-instruct\n",
      " - microsoft/phi-3-mini-4k-instruct\n",
      " - microsoft/phi-3-small-128k-instruct\n",
      " - microsoft/phi-3-small-8k-instruct\n",
      " - microsoft/phi-3-vision-128k-instruct\n",
      " - microsoft/phi-3.5-mini-instruct\n",
      " - microsoft/phi-3.5-moe-instruct\n",
      " - microsoft/phi-3.5-vision-instruct\n",
      " - microsoft/phi-4-mini-flash-reasoning\n",
      " - microsoft/phi-4-mini-instruct\n",
      " - microsoft/phi-4-multimodal-instruct\n",
      " - mistralai/codestral-22b-instruct-v0.1\n",
      " - mistralai/magistral-small-2506\n",
      " - mistralai/mamba-codestral-7b-v0.1\n",
      " - mistralai/mathstral-7b-v0.1\n",
      " - mistralai/mistral-7b-instruct-v0.2\n",
      " - mistralai/mistral-7b-instruct-v0.3\n",
      " - mistralai/mistral-large\n",
      " - mistralai/mistral-large-2-instruct\n",
      " - mistralai/mistral-medium-3-instruct\n",
      " - mistralai/mistral-nemotron\n",
      " - mistralai/mistral-small-24b-instruct\n",
      " - mistralai/mistral-small-3.1-24b-instruct-2503\n",
      " - mistralai/mixtral-8x22b-instruct-v0.1\n",
      " - mistralai/mixtral-8x22b-v0.1\n",
      " - mistralai/mixtral-8x7b-instruct-v0.1\n",
      " - moonshotai/kimi-k2-instruct\n",
      " - nv-mistralai/mistral-nemo-12b-instruct\n",
      " - nvidia/embed-qa-4\n",
      " - nvidia/llama-3.1-nemoguard-8b-content-safety\n",
      " - nvidia/llama-3.1-nemoguard-8b-topic-control\n",
      " - nvidia/llama-3.1-nemotron-51b-instruct\n",
      " - nvidia/llama-3.1-nemotron-70b-instruct\n",
      " - nvidia/llama-3.1-nemotron-70b-reward\n",
      " - nvidia/llama-3.1-nemotron-nano-4b-v1.1\n",
      " - nvidia/llama-3.1-nemotron-nano-8b-v1\n",
      " - nvidia/llama-3.1-nemotron-nano-vl-8b-v1\n",
      " - nvidia/llama-3.1-nemotron-ultra-253b-v1\n",
      " - nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1\n",
      " - nvidia/llama-3.2-nemoretriever-300m-embed-v1\n",
      " - nvidia/llama-3.2-nv-embedqa-1b-v1\n",
      " - nvidia/llama-3.2-nv-embedqa-1b-v2\n",
      " - nvidia/llama-3.3-nemotron-super-49b-v1\n",
      " - nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      " - nvidia/llama3-chatqa-1.5-70b\n",
      " - nvidia/llama3-chatqa-1.5-8b\n",
      " - nvidia/mistral-nemo-minitron-8b-8k-instruct\n",
      " - nvidia/mistral-nemo-minitron-8b-base\n",
      " - nvidia/nemoretriever-parse\n",
      " - nvidia/nemotron-4-340b-instruct\n",
      " - nvidia/nemotron-4-340b-reward\n",
      " - nvidia/nemotron-4-mini-hindi-4b-instruct\n",
      " - nvidia/nemotron-mini-4b-instruct\n",
      " - nvidia/neva-22b\n",
      " - nvidia/nv-embed-v1\n",
      " - nvidia/nv-embedcode-7b-v1\n",
      " - nvidia/nv-embedqa-e5-v5\n",
      " - nvidia/nv-embedqa-mistral-7b-v2\n",
      " - nvidia/nvclip\n",
      " - nvidia/riva-translate-4b-instruct\n",
      " - nvidia/usdcode-llama-3.1-70b-instruct\n",
      " - nvidia/vila\n",
      " - openai/gpt-oss-120b\n",
      " - openai/gpt-oss-20b\n",
      " - opengpt-x/teuken-7b-instruct-commercial-v0.4\n",
      " - qwen/qwen2-7b-instruct\n",
      " - qwen/qwen2.5-7b-instruct\n",
      " - qwen/qwen2.5-coder-32b-instruct\n",
      " - qwen/qwen2.5-coder-7b-instruct\n",
      " - qwen/qwen3-235b-a22b\n",
      " - qwen/qwq-32b\n",
      " - rakuten/rakutenai-7b-chat\n",
      " - rakuten/rakutenai-7b-instruct\n",
      " - sarvamai/sarvam-m\n",
      " - snowflake/arctic-embed-l\n",
      " - speakleash/bielik-11b-v2.3-instruct\n",
      " - thudm/chatglm3-6b\n",
      " - tiiuae/falcon3-7b-instruct\n",
      " - tokyotech-llm/llama-3-swallow-70b-instruct-v0.1\n",
      " - upstage/solar-10.7b-instruct\n",
      " - utter-project/eurollm-9b-instruct\n",
      " - writer/palmyra-creative-122b\n",
      " - writer/palmyra-fin-70b-32k\n",
      " - writer/palmyra-med-70b\n",
      " - writer/palmyra-med-70b-32k\n",
      " - yentinglin/llama-3-taiwan-70b-instruct\n",
      " - zyphra/zamba2-7b-instruct\n",
      "\n",
      "Example Entry:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'mistralai/mixtral-8x7b-instruct-v0.1',\n",
       " 'object': 'model',\n",
       " 'created': 735790403,\n",
       " 'owned_by': 'mistralai'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "invoke_url = \"http://llm_client:9000/v1/models\"\n",
    "# invoke_url = \"https://api.openai.com/v1/models\"\n",
    "# invoke_url = \"https://integrate.api.nvidia.com/v1\"\n",
    "# invoke_url = \"http://llm_client:9000/v1/models/mistralai/mixtral-8x7b-instruct-v0.1\"\n",
    "# invoke_url = \"http://llm_client:9000/v1/models/mistralaimixtral-8x7b-instruct-v0.1\"\n",
    "headers = {\n",
    "    \"content-type\": \"application/json\",\n",
    "    # \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
    "    # \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\",\n",
    "}\n",
    "\n",
    "print(\"Available Models:\")\n",
    "response = requests.get(invoke_url, headers=headers, stream=False)\n",
    "print(response)\n",
    "# print(response.json())  ## <- Raw Response. Very Verbose\n",
    "for model_entry in response.json().get(\"data\", []):\n",
    "    print(\" -\", model_entry.get(\"id\"))\n",
    "\n",
    "print(\"\\nExample Entry:\")\n",
    "invoke_url = \"http://llm_client:9000/v1/models/mistralai/mixtral-8x7b-instruct-v0.1\"\n",
    "requests.get(invoke_url, headers=headers, stream=False).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e17d4-0b98-485c-b80c-7d2922f00631",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "We will not be operating much on this level of abstraction for this course, but it's worth going through the basic process to confirm that, yes, these requests are coming through our microservice in pretty much the same way as if the server were hosted remotely. You can assume for the remainder of the course that an interaction like the following is taking place under the hood of your clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed43fee2-0434-4709-b61f-2df26160bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "## Where are you sending your requests?\n",
    "invoke_url = \"http://llm_client:9000/v1/chat/completions\"\n",
    "\n",
    "## If you wanted to use your own API Key, it's very similar\n",
    "# if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "#     os.environ[\"NVIDIA_API_KEY\"] = getpass(\"NVIDIA_API_KEY: \")\n",
    "# invoke_url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "\n",
    "## If you wanted to use OpenAI, it's very similar\n",
    "# if not os.environ.get(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"):\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass(\"OPENAI_API_KEY: \")\n",
    "# invoke_url = \"https://api.openai.com/v1/models\"\n",
    "\n",
    "## Meta communication-level info about who you are, what you want, etc.\n",
    "headers = {\n",
    "    \"accept\": \"text/event-stream\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    # \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
    "    # \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\",\n",
    "}\n",
    "\n",
    "## Arguments to your server function\n",
    "payload = {\n",
    "    \"model\": \"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "    \"messages\": [{\"role\":\"user\",\"content\":\"Tell me hello in French\"}],\n",
    "    \"temperature\": 0.5,   \n",
    "    \"top_p\": 1,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True                \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f56d6937-449d-465d-a257-a6e88ab7f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In French, \"hello\" can be translated as \"bonjour\" (pronounced: bohn-zhoor). This is a formal way to greet someone during the daytime. If you're saying hello in the evening, you can use \"bonsoir\" (pronounced: bohn-swahr). For a casual or informal setting, you can use \"salut\" (pronounced: sah-luu), which can be used for both greetings and farewells.\n",
      "\n",
      "Keep in mind that the pronunciation may vary slightly depending on the region and the speaker's accent. It's always a good idea to listen to native speakers and practice speaking the language to improve your pronunciation."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "## Use requests.post to send the header (streaming meta-info) the payload to the endpoint\n",
    "## Make sure streaming is enabled, and expect the response to have an iter_lines response.\n",
    "response = requests.post(invoke_url, headers=headers, json=payload, stream=True)\n",
    "\n",
    "## If your response is an error message, this will raise an exception in Python\n",
    "try: \n",
    "    response.raise_for_status()  ## If not 200 or similar, this will raise an exception\n",
    "except Exception as e:\n",
    "    # print(response.json())\n",
    "    print(response.json())\n",
    "    raise e\n",
    "\n",
    "## Custom utility to make live a bit easier\n",
    "def get_stream_token(entry: bytes):\n",
    "    \"\"\"Utility: Coerces out ['choices'][0]['delta'][content] from the bytestream\"\"\"\n",
    "    if not entry: return \"\"\n",
    "    entry = entry.decode('utf-8')\n",
    "    if entry.startswith('data: '):\n",
    "        try: entry = json.loads(entry[5:])\n",
    "        except ValueError: return \"\"\n",
    "    return entry.get('choices', [{}])[0].get('delta', {}).get('content') or \"\"\n",
    "\n",
    "## If the post request is honored, you should be able to iterate over \n",
    "for line in response.iter_lines():\n",
    "    \n",
    "    ## Without Processing: data: {\"id\":\"...\", ... \"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"}...}...\n",
    "    # if line: print(line.decode(\"utf-8\"))\n",
    "\n",
    "    ## With Processing: An actual stream of tokens printed one-after-the-other as they come in\n",
    "    print(get_stream_token(line), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d6938-304d-465c-be29-6b95c79ef76c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **[NOTES]**\n",
    "\n",
    "**You may notice that the chat models expect \"messages\" as input:**\n",
    "\n",
    "This may be unexpected if you're more used to raw LLM interfaces like those of local HuggingFace models, but it will look pretty standard to users of OpenAI models. By enforcing a restricted interface instead of a raw text completion one, the service can have more control over what the users can do. There are plenty of pros and cons to this interface, with some noteworthy ones below:\n",
    "- A service might restrict the use of a specific role type or parameter (i.e. system message restriction, priming message to get arbitrary generation, etc).\n",
    "- A service might enforce custom prompt formats and implement extra options under the hood that rely on the chat interface.\n",
    "- A service might use stronger assumptions to implement deeper optimizations in the inference pipeline.\n",
    "- A service might mimic another popular interface to leverage existing ecosystem compatibilities.\n",
    "\n",
    "All of these are valid reasons, and it's important to consider which interface options are best for your particular use cases when choosing or deploying your own service.\n",
    "\n",
    "**You may notice that there are two fundamental ways of querying the models:**\n",
    "\n",
    "You can **invoke without streaming**, in which case the service response will come all at once after it has been computed in full. This is great when you need the entire output of the model before doing anything else; for example, when you want to print out the whole result or use it for downstream tasks. The response body will look something like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": \"d34d436a-c28b-4451-aa9c-02eed2141ed3\",\n",
    "    \"choices\": [{\n",
    "        \"index\": 0,\n",
    "        \"message\": { \"role\": \"assistant\", \"content\": \"Bonjour! ...\" },\n",
    "        \"finish_reason\": \"stop\"\n",
    "    }],\n",
    "    \"usage\": {\n",
    "        \"completion_tokens\": 450,\n",
    "        \"prompt_tokens\": 152,\n",
    "        \"total_tokens\": 602\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "You can also **invoke with streaming**, in which case the service will send out a series of requests until a final request is sent out. This is great when you can use the responses of the service as it becomes available (which is very good for language model components that print the output directly to the user as it gets generated). In this case, the response body will look a lot more like this:\n",
    "\n",
    "```json\n",
    "data:{\"id\":\"...\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"Bon\"},\"finish_reason\":null}]}\n",
    "data:{\"id\":\"...\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"j\"},\"finish_reason\":null}]}\n",
    "...\n",
    "data:{\"id\":\"...\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":\"stop\"}]}\n",
    "data:[DONE]\n",
    "```\n",
    "\n",
    "Both of these options can be done with relative ease using Python's `requests` library, but using the interface as-is will result in a lot of repetitive code. Luckily, we have some systems that make this significantly easier to use and incorporate into larger projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f13e0-cc18-4b67-911a-27813b1dd0df",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### **4.2.** OpenAI Client Request\n",
    "\n",
    "It's good to know that this interface exists, but using it as-is will result in a lot of repetitive code and extra complexity. Luckily, we have some systems that make this significantly easier to use and incorporate into larger projects! One layer of abstraction above using the requests is to use a more opinionated client like that of OpenAI. Since both NVIDIA and OpenAI subscribe to the OpenAPI specification, we can borrow their client instead. Note that under the hood, the same processes are still being done, probably facilitated by a lower-level client like that of [**httpx**](https://github.com/projectdiscovery/httpx) or [**aiohttp**](https://github.com/aio-libs/aiohttp). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f324d88-94e0-480a-9ad6-2ab1aed3eab4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import and initialize the OpenAI client if not already done\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m----> 4\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://llm_client:9000/v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# api_key=\"YOUR_API_KEY\",  # Not needed for local llm_client\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/mixtral-8x7b-instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# model=\"gpt-4-turbo-2024-04-09\",\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m## Streaming with Generator: Results come out as they're generated\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_client.py:130\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    128\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m     )\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "## Using General OpenAI Client\n",
    "from openai import OpenAI\n",
    "\n",
    "# client = OpenAI()  ## Assumes OPENAI_API_KEY is set\n",
    "\n",
    "# client = OpenAI(\n",
    "#     base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "#     api_key = os.environ.get(\"NVIDIA_API_KEY\", \"\")\n",
    "# )\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"http://llm_client:9000/v1\",\n",
    "    api_key = \"I don't have one\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "    # model=\"gpt-4-turbo-2024-04-09\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Hello World\"}],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "## Streaming with Generator: Results come out as they're generated\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f17380f-eb8f-4cf8-9589-4d8300cb611c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='cmpl-63ffa779ac054c78b410ceae26b1ce3e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! It\\'s nice to meet you. \"Hello World\" is often the first program that people write when they are learning a new programming language. It\\'s a simple program that outputs the text \"Hello World\" to the console. I\\'m here to help answer any questions you have about programming or technology, so feel free to ask me anything!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), stop_reason=None)], created=1755202572, model='mistralai/mixtral-8x7b-instruct-v0.1', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=73, prompt_tokens=11, total_tokens=84, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Non-Streaming: Results come from server when they're all ready\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "    # model=\"gpt-4-turbo-2024-04-09\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Hello World\"}],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090b044-0880-479b-91bf-3deb2b0a0e46",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### **4.3.** ChatNVIDIA Client Request\n",
    "\n",
    "So far, we've seen communication happen on two layers of abstraction: **raw requests** and **API client**. In this course, we will want to do LLM orchestration with a framework called LangChain, so we'll need to go one layer of abstraction higher to a **Framework Connector**.\n",
    "\n",
    "The goal of a **connector** is to convert an arbitrary API from its native core into one that a target code-base would expect. In this course, we'll want to take advantage of LangChain's thriving chain-centric ecosystem, but the raw `requests` API will not take us all the way there. Under the hood, every LangChain chat model that isn't hosted locally has to rely on such an API, but the developer-facing API is a much cleaner [`LLM` or `SimpleChatModel`-style interface](https://python.langchain.com/v0.1/docs/modules/model_io/) with default parameters and some simple utility functions like `invoke` and `stream`.\n",
    "\n",
    "To start off our exploration into the LangChain interface, we will use the `ChatNVIDIA` connector to interface with our `chat/completions` endpoints. This model is part of the LangChain extended ecosystem and can be installed locally via `pip install langchain-nvidia-ai-endpoints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d36f5990-8f3e-4359-b46f-2deeaad6b0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! It\\'s nice to meet you. \"Hello World\" is often the first program that people write when they are learning a new programming language. It\\'s a simple program that outputs the text \"Hello World\" to the console. I\\'m here to help answer any questions you have about programming or technology, so feel free to ask me anything!', additional_kwargs={}, response_metadata={'role': 'assistant', 'content': 'Hello! It\\'s nice to meet you. \"Hello World\" is often the first program that people write when they are learning a new programming language. It\\'s a simple program that outputs the text \"Hello World\" to the console. I\\'m here to help answer any questions you have about programming or technology, so feel free to ask me anything!', 'token_usage': {'prompt_tokens': 11, 'total_tokens': 84, 'completion_tokens': 73}, 'finish_reason': 'stop', 'model_name': 'mistralai/mixtral-8x7b-instruct-v0.1'}, id='run--fc722ae0-526c-4f60-a486-85dd46c29d56-0', usage_metadata={'input_tokens': 11, 'output_tokens': 73, 'total_tokens': 84}, role='assistant')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "## NVIDIA_API_KEY pulled from environment\n",
    "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "# llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", mode=\"open\", base_url=\"http://llm_client:9000/v1\")\n",
    "llm.invoke(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cbfe72a-b69b-4fe1-8d21-bf45c1ba6789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'http://llm_client:9000/v1/chat/completions',\n",
       " 'headers': {'Accept': 'application/json',\n",
       "  'Authorization': 'Bearer **********',\n",
       "  'User-Agent': 'langchain-nvidia-ai-endpoints'},\n",
       " 'json': {'messages': [{'role': 'user', 'content': 'Hello World'}],\n",
       "  'model': 'mistralai/mixtral-8x7b-instruct-v0.1',\n",
       "  'max_tokens': 1024,\n",
       "  'stream': False}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._client.last_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a82418e9-71fd-45ab-a558-9c17ea696864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-a4203db8beb144218acdbc922da26308',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1755202869,\n",
       " 'model': 'mistralai/mixtral-8x7b-instruct-v0.1',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Hello! It\\'s nice to meet you. \"Hello World\" is often the first program that people write when they are learning a new programming language. It\\'s a simple program that outputs the text \"Hello World\" to the console. I\\'m here to help answer any questions you have about programming or technology, so feel free to ask me anything!'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'stop_reason': None}],\n",
       " 'usage': {'prompt_tokens': 11, 'total_tokens': 84, 'completion_tokens': 73}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm._client.last_response\n",
    "llm._client.last_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240e9b1-7ee6-400a-b0ab-5f24950b04af",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### **[NOTES]**\n",
    "\n",
    "- **The course uses a modified fork of the `ai-endpoints` connector with several features which are more useful for our course environment.** These features are not yet in the main version and are being proactively incorporated alongside other developments and requirements from the [**LlamaIndex**](https://docs.llamaindex.ai/en/stable/examples/embeddings/nvidia/) and [**Haystack**](https://docs.haystack.deepset.ai/docs/nvidiagenerator) counterparts. \n",
    "\n",
    "- **ChatNVIDIA is defaulting to the `llm_client` microservice because we set some environment variables to make it happen**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4cd957f-28b5-4089-8c63-68b6875248b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NVIDIA_DEFAULT_MODE': 'open', 'NVIDIA_BASE_URL': 'http://llm_client:9000/v1'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "{k:v for k,v in os.environ.items() if k.startswith(\"NVIDIA_\")}\n",
    "## Translation: Use the base_url of llm_client:9000 for the requests,\n",
    "## and use \"open\"api-spec access for model discovery and url formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035197d8-384c-4715-acb5-49d711f7673c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "**Throughout the course, feel free to try out the model of your choice.** Below are the selection of models provided as part of this course, out of which a selection should be working at any given time.\n",
    "\n",
    "You are also free to try other models, though you may need to upgrade the `langchain_nvidia_ai_endpoints` library and provide your own key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a9f0ab9-0751-4fd5-a44b-16ae2c4c18f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:237: UserWarning: Default model is set as: 01-ai/yi-large. \n",
      "Set model using model parameter. \n",
      "To get available models use available_models property.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL: meta/llama-3.1-70b-instruct\n",
      "I'm an artificial intelligence language model, which means I'm a computer program designed to understand and respond to human language, generating text based on the input I receive. I'm a large language model, I don't have a personal history or physical presence, but I'm always happy to chat, answer questions, and provide information to the best of my abilities!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.1-8b-instruct\n",
      "I'm an artificial intelligence designed to assist and communicate with humans, and I don't have a physical body or personal identity, but I can provide information and answer questions on a wide range of topics. I'm a large language model, trained on a massive dataset of text to generate human-like responses, and I'm here to help with any questions or topics you'd like to discuss!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.2-11b-vision-instruct\n",
      "I am an artificial intelligence language model designed to assist and communicate with users in a helpful and informative manner. I don't have personal experiences or emotions, but I can provide information, answer questions, and engage in conversation to the best of my abilities based on my training data.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.2-1b-instruct\n",
      "I'm an artificial intelligence designed to assist and communicate with humans in a helpful and informative way. I don't have a personal identity or experiences, but I'm here to help answer your questions and provide insights on a wide range of topics!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.2-3b-instruct\n",
      "I'm an artificial intelligence model designed to provide information, answer questions, and engage in conversation to the best of my knowledge and abilities. I don't have personal experiences, emotions, or opinions, but I'm here to help you with any topic or question you'd like to discuss.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.2-90b-vision-instruct\n",
      "I'm an artificial intelligence model designed to assist and communicate with users in a helpful and informative way, providing knowledge and answering questions on a wide range of topics. I don't have personal experiences or emotions, but I'm here to help you with any information or tasks you need assistance with.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.3-70b-instruct\n",
      "I'm an artificial intelligence language model, which means I'm a computer program designed to understand and respond to human language in a way that simulates conversation and answers questions to the best of my knowledge. I don't have a physical body or personal experiences, but I'm constantly learning and improving to provide helpful and accurate information on a wide range of topics!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-4-maverick-17b-128e-instruct\n",
      "I'm an AI assistant designed to help users with various tasks and questions, providing information and insights to the best of my abilities. I don't have a personal identity or emotions, but I'm here to assist and provide helpful responses to your queries.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-4-scout-17b-16e-instruct\n",
      "Stopped manually\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "model_list = ChatNVIDIA.get_available_models()\n",
    "\n",
    "for model_card in model_list:\n",
    "    model_name = model_card.id\n",
    "    ## If you want to, might be a good idea to not go through EVERY model\n",
    "    if not any([keyword in model_name for keyword in [\"meta/llama\"]]): continue\n",
    "    if \"405b\" in model_name: continue\n",
    "    if \"embed\" in model_name: continue\n",
    "    \n",
    "    llm = ChatNVIDIA(model=model_name)\n",
    "    print(f\"TRIAL: {model_name}\")\n",
    "    try: \n",
    "        for token in llm.stream(\"Tell me about yourself! 2 sentences.\", max_tokens=100):\n",
    "            print(token.content, end=\"\")\n",
    "    except Exception as e: \n",
    "        print(f\"EXCEPTION: {e}\")    ## If some models fail, feel free to use others\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"Stopped manually\")  ## Feel free to hit square while running\n",
    "        break\n",
    "    print(\"\\n\\n\" + \"=\"*84)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aWLCNC3fAsVx",
   "metadata": {
    "id": "aWLCNC3fAsVx"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5:** Wrap-Up\n",
    "\n",
    "The goal of this notebook was to provide some discussion centered around LLM service hosting strategies and introduce you to the AI Foundation Model endpoints. Along the way, we hope that you have an intuitive understanding of how remote LLM systems can be provided and accessed from edge devices!\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffac91-77fa-4a6c-a35a-a970fb652617",
   "metadata": {
    "id": "25ffac91-77fa-4a6c-a35a-a970fb652617"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
