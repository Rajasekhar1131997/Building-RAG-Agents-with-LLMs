{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d391c2-0c25-4f4a-9ee1-4bcca57a3d96",
   "metadata": {
    "id": "90d391c2-0c25-4f4a-9ee1-4bcca57a3d96"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee3921-2244-4545-b0df-0b0ebebff32d",
   "metadata": {
    "id": "38ee3921-2244-4545-b0df-0b0ebebff32d"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 9:** LangServe and Assessment</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "## LangServe Server Setup\n",
    "\n",
    "This notebook is a playground for those interested in developing interactive web applications using LangChain and [**LangServe**](https://python.langchain.com/docs/langserve). The aim is to provide a minimal-code example to illustrate the potential of LangChain in web application contexts.\n",
    "\n",
    "This section provides a walkthrough for setting up a simple API server using LangChain's Runnable interfaces with FastAPI. The example demonstrates how to integrate a LangChain model, such as `ChatNVIDIA`, to create and distribute accessible API routes. Using this, you will be able to supply functionality to the frontend service's [**`frontend_server.py`**](frontend/frontend_server.py) session, which strongly expects:\n",
    "- A simple endpoint named `:9012/basic_chat` for the basic chatbot, exemplified below.\n",
    "- A pair of endpoints named `:9012/retriever` and `:9012/generator` for the RAG chatbot.\n",
    "- All three for the **Evaluate** utility, which will be required for the final assessment. *More on that later!*\n",
    "\n",
    "**IMPORTANT NOTES:**\n",
    "- Make sure to click the square ( $\\square$ ) button twice to shut down an active FastAPI cell. The first time might fall through or trigger a try-catch routine on an asynchronous process.\n",
    "- If it still doesn't work, do a hard restart on this notebook by using **Kernel -> Restart Kernel**.\n",
    "- When a FastAPI server is running in your cell, expect the process to block up this notebook. Other notebooks should not be impacted by this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YRX1R3GupzkZ",
   "metadata": {
    "id": "YRX1R3GupzkZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Part 1:** Delivering the /basic_chat endpoint\n",
    "\n",
    "Instructions are provided for launching a `/basic_chat` endpoint both as a standalone Python file. This will be used by the frontend to make basic decision with no internal reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TniVLtL-qcqo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1702915515784,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "TniVLtL-qcqo",
    "outputId": "7ff6eb58-b9c1-4ce9-b15a-b1a515045ae0"
   },
   "outputs": [],
   "source": [
    "%%writefile server_app.py\n",
    "from fastapi import FastAPI\n",
    "from operator import itemgetter\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "\n",
    "from langserve import add_routes\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"nvidia/nv-embed-v1\")   # must match index build model\n",
    "LLM_MODEL   = os.getenv(\"LLM_MODEL\", \"meta/llama-3.1-8b-instruct\")\n",
    "INDEX_DIR   = os.getenv(\"INDEX_DIR\", \"docstore_index\")\n",
    "PORT        = int(os.getenv(\"PORT\", \"9012\"))\n",
    "\n",
    "# -------------- Utilities ---------------\n",
    "def normalize_input(x: Any) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Accept raw strings or dicts and always return a dict with at least 'input'.\n",
    "    Preserves extra keys like 'context' or 'docs' if provided.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        return {\"input\": x}\n",
    "    if isinstance(x, dict):\n",
    "        if \"input\" in x and isinstance(x[\"input\"], str):\n",
    "            return x\n",
    "        # common alternates some UIs send\n",
    "        for k in (\"question\", \"query\", \"text\", \"prompt\"):\n",
    "            if k in x and isinstance(x[k], str):\n",
    "                return {\"input\": x[k], **{kk: vv for kk, vv in x.items() if kk != k}}\n",
    "        # last resort: stringify\n",
    "        return {\"input\": str(x), **x}\n",
    "    return {\"input\": str(x)}\n",
    "\n",
    "normalize = RunnableLambda(normalize_input)\n",
    "\n",
    "# ----------- Load FAISS & LLM -----------\n",
    "if not os.path.isdir(INDEX_DIR):\n",
    "    raise FileNotFoundError(f\"Missing '{INDEX_DIR}/'. Make sure your FAISS index is present.\")\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=EMBED_MODEL, truncate=\"END\")\n",
    "docstore = FAISS.load_local(INDEX_DIR, embedder, allow_dangerous_deserialization=True)\n",
    "retriever = docstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "llm = ChatNVIDIA(model=LLM_MODEL) | StrOutputParser()\n",
    "\n",
    "# ---------------- Prompts ----------------\n",
    "basic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise, helpful assistant.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Answer ONLY using the provided context. If the context is insufficient, say you don't know.\\n\\n\"\n",
    "     \"Context:\\n{context}\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# ---------------- Chains -----------------\n",
    "# /basic_chat: accept string or dict\n",
    "basic_chat_chain = normalize | basic_prompt | llm\n",
    "\n",
    "# /retriever: MUST return List[Document] (no reorder/docs2str here; frontend does it)\n",
    "retriever_chain = normalize | itemgetter(\"input\") | retriever\n",
    "\n",
    "# /generator: expects {\"input\": \"...\", \"context\": \"<string>\"}\n",
    "# (frontend builds the context string for you)\n",
    "generator_chain = normalize | rag_prompt | llm\n",
    "\n",
    "# --------------- App ---------------------\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"RAG endpoints compatible with the course frontend/grader.\"\n",
    ")\n",
    "\n",
    "add_routes(app, basic_chat_chain, path=\"/basic_chat\")\n",
    "add_routes(app, retriever_chain,  path=\"/retriever\")\n",
    "add_routes(app, generator_chain,  path=\"/generator\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"status\": \"ok\",\n",
    "            \"routes\": [\"/basic_chat/invoke\", \"/basic_chat/stream\",\n",
    "                       \"/retriever/invoke\", \"/retriever/stream\",\n",
    "                       \"/generator/invoke\", \"/generator/stream\"]}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u2xDAYn1qi_D",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2xDAYn1qi_D",
    "outputId": "ef35c8f4-210c-4c10-82e5-a3de2bfe1835"
   },
   "outputs": [],
   "source": [
    "## Works, but will block the notebook.\n",
    "!python server_app.py  \n",
    "\n",
    "## Will technically work, but not recommended in a notebook. \n",
    "## You may be surprised at the interesting side effects...\n",
    "# import os\n",
    "# os.system(\"python server_app.py &\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g9uRMEOrsy1d",
   "metadata": {
    "id": "g9uRMEOrsy1d"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Part 2:** Using The Server:\n",
    "\n",
    "While this cannot be easily utilized within Google Colab (or at least not without a lot of special tricks), the above script will keep a running server tied to the notebook process. While the server is running, do not attempt to use this notebook (except to shut down/restart the service).\n",
    "\n",
    "In another file, however, you should be able to access the `basic_chat` endpoint using the following interface:\n",
    "\n",
    "```python\n",
    "from langserve import RemoteRunnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = RemoteRunnable(\"http://0.0.0.0:9012/basic_chat/\") | StrOutputParser()\n",
    "for token in llm.stream(\"Hello World! How is it going?\"):\n",
    "    print(token, end='')\n",
    "```\n",
    "\n",
    "**Please try it out in a different file and see if it works!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d5501-b916-4045-bb61-b9f35ecad5df",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Part 3: Final Assessment**\n",
    "\n",
    "**This notebook will be used to completing the final assessment!** When you have otherwise finished the course, we recommend cloning this notebook, getting the frontend open in a new tab, and implement the Evaluate functionality by implementing the `/generator` and `/retriever` endpoints above! For a quick link to the frontend, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24beaf98-fb5e-477d-a711-5204f1cb4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:#76b900;\" target=\"_blank\" href='+url+'><h2>< Link To Gradio Frontend ></h2></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbff129",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "#### **Assessment Hint:** \n",
    "Note that the following functionality is already implemented in the frontend microservice. \n",
    "\n",
    "```python\n",
    "## Necessary Endpoints\n",
    "chains_dict = {\n",
    "    'basic' : RemoteRunnable(\"http://lab:9012/basic_chat/\"),\n",
    "    'retriever' : RemoteRunnable(\"http://lab:9012/retriever/\"),  ## For the final assessment\n",
    "    'generator' : RemoteRunnable(\"http://lab:9012/generator/\"),  ## For the final assessment\n",
    "}\n",
    "\n",
    "basic_chain = chains_dict['basic']\n",
    "\n",
    "## Retrieval-Augmented Generation Chain\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign(\n",
    "        {'context' : itemgetter('input') \n",
    "        | chains_dict['retriever'] \n",
    "        | LongContextReorder().transform_documents\n",
    "        | docs2str\n",
    "    })\n",
    ")\n",
    "\n",
    "output_chain = RunnableAssign({\"output\" : chains_dict['generator'] }) | output_puller\n",
    "rag_chain = retrieval_chain | output_chain\n",
    "```\n",
    "\n",
    "**To conform to this endpoint ingestion strategy, make sure not to duplicate pipeline functionality and only deploy the features that are missing!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c72a8e-3b5b-4442-a6aa-b94b839cacb2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c642796-2fc1-4ddf-a96a-35c0dbb3a54a",
   "metadata": {
    "id": "2c642796-2fc1-4ddf-a96a-35c0dbb3a54a"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
